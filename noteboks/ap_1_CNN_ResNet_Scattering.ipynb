{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ap_1_CNN_ResNet_Scattering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RPaF-XBvdS1M"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHSKTXBmThZJ"
      },
      "source": [
        "# RecVis Assigment 3 \n",
        "\n",
        "**Author:** Arthur Pignet\n",
        "arthur.pignet@mines-paristech.fr\n",
        "\n",
        "This notebook is the first of the two notebook I did for this assigment\n",
        "\n",
        "It will cover the native CNN training, finetuning of a ResNet pretrained on ImageNet and a features extraction with scattering wavelet network followed by a CNN.\n",
        "\n",
        "The second notebook cover another approach with feature extraction via an autoencoder trained on unlabelled data. \n",
        "\n",
        "The notebook has been run with Google Colab, on P100 GPUs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvUzer_oY9yD"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7hYPfPCYqOw"
      },
      "source": [
        "!git config --global user.email \"arthur.pignet@mines-paristech.fr\"\n",
        "!git config --global user.name \"arthurPignet\"\n",
        "!git clone https://github.com/arthurPignet/recvis21_a3.git\n",
        "%cd recvis21_a3\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsBCvJFGZd4Y"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR47oGI9aFkq"
      },
      "source": [
        "# get the data\n",
        "!wget -q https://www.di.ens.fr/willow/teaching/recvis18orig/assignment3/bird_dataset.zip\n",
        "!unzip bird_dataset.zip \n",
        "!rm bird_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPaF-XBvdS1M"
      },
      "source": [
        "### Standart model\n",
        "\n",
        "This is the native model.\n",
        "In practice I did not submitted the test prediction to Kaggle as it would have been a wasted submission token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLyevWhZ-UN"
      },
      "source": [
        "!python main.py --experiment 'experiment_0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2GKJbk5a-zv"
      },
      "source": [
        "!python evaluate.py --model experiment_0/model_10.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfMM4el2cl5T"
      },
      "source": [
        "!git add experiment_0/*\n",
        "!git commit -m 'add experiment_0, with the given model and parameters'\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1JgxAQVdBtu"
      },
      "source": [
        "## ResNet Finetuning\n",
        "\n",
        "For this section I heavily relied on the associated pytorch tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP1iVeQOTJBv"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259lnKUqFKJW"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix \n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from src.models import initialize_pretrained_model\n",
        "from src.data import get_data_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXnazxC4gTWW"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    print('Using GPU')\n",
        "    model.cuda()\n",
        "else:\n",
        "    print('Using CPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcSwGeylTL1I"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaFrC3-jcudK"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./bird_dataset/\"\n",
        "experiment = './experiment_resnet'\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 20\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 100\n",
        "\n",
        "lr = 0.01\n",
        "momentum = 0.8\n",
        "\n",
        "log_interval = 2\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBCh-vD2TPWB"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfJwtD9mA7-2"
      },
      "source": [
        "model, input_size =  initialize_pretrained_model(model_name, num_classes, feature_extract, use_pretrained=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Y1KIU4Nubf"
      },
      "source": [
        "model.cuda()\n",
        "summary(model, (3,input_size,input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FUsSfB6TRKM"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTIgsHfNdshd"
      },
      "source": [
        "# data transform\n",
        "data_transform = get_data_transform(input_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(data_dir + '/train_images',\n",
        "                         transform=data_transforms['train']),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(data_dir + '/val_images',\n",
        "                         transform=data_transforms['val']),\n",
        "    batch_size=batch_size, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhngEjM0TTN1"
      },
      "source": [
        "###training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEVlZNw2vPOF"
      },
      "source": [
        "# Neural network and optimizer\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "        validation_loss += criterion(output, target).data.item()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "    \n",
        "    return validation_loss, 100. * correct / len(val_loader.dataset)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_oWFPT3dreR"
      },
      "source": [
        "best_epoch = 0\n",
        "best_loss = 100000\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    val_loss, accuracy = validation()\n",
        "    if val_loss < best_loss:\n",
        "      patience = 0\n",
        "      best_epoch = epoch\n",
        "      best_loss = val_loss\n",
        "      best_state = model.state_dict()\n",
        "    else:\n",
        "      patience += 1\n",
        "\n",
        "    if patience > 4:\n",
        "      break\n",
        "      #model_file = experiment + '/model_' + str(epoch) + '.pth'\n",
        "      #torch.save(model.state_dict(), model_file)\n",
        "      #print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')\n",
        "    if epoch % 10 == 0:\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n",
        "                                    weight_decay=0.0005)\n",
        "      lr*=0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpovYYu8p9ML"
      },
      "source": [
        "#state_dict = torch.load(model_file)\n",
        "model.load_state_dict(best_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdoaVZ0YOG59"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "The following script is here to evaluate the performance of the model per class on the validation set, in order to detect unbalanced validation dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEHS9qeL8TFC"
      },
      "source": [
        "model.eval()\n",
        "validation_loss = 0\n",
        "correct = 0\n",
        "n_target = []\n",
        "n_output = []\n",
        "for data, target in val_loader:\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    output = model(data)\n",
        "    n_target += list(target.cpu().detach().numpy()) \n",
        "    n_output += list(np.argmax(output.cpu().detach().numpy(), axis=1))\n",
        "    # sum up batch loss\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "    validation_loss += criterion(output, target).data.item()\n",
        "    # get the index of the max log-probability\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "validation_loss /= len(val_loader.dataset)\n",
        "print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "    validation_loss, correct, len(val_loader.dataset),\n",
        "    100. * correct / len(val_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w3J9sYRvMF5"
      },
      "source": [
        "conf = confusion_matrix(n_target, n_output)\n",
        "sns.heatmap(conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pd1tenSSo6W"
      },
      "source": [
        "### Kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhhZwR0SoSD"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "from src.utils import pil_loader\n",
        "\n",
        "model.eval()\n",
        "test_dir = data_dir + '/test_images/mistery_category'\n",
        "\n",
        "output_file = open(f'{experiment}/result_resnet.csv', \"w\")\n",
        "output_file.write(\"Id,Category\\n\")\n",
        "for f in tqdm(os.listdir(test_dir)):\n",
        "    if 'jpg' in f:\n",
        "        data = data_transforms['val'](pil_loader(test_dir + '/' + f))\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "        sc = scattering(data)\n",
        "        output = model(sc)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
        "\n",
        "output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DubWy6JJSyLh"
      },
      "source": [
        "!git add experiment_resnet/result_resnet.csv\n",
        "!git commit -m 'add experiment with resnet finetune'\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agljfl-G-pfM"
      },
      "source": [
        "## Scattering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGIoMcp4SevD"
      },
      "source": [
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FS-Vx4dOmXG"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from kymatio.torch import Scattering2D\n",
        "from src.models import CNNScattering \n",
        "from src.data import get_data_transform\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQwa1QoSE1G"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nty86S9AjmD"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./bird_dataset/\"\n",
        "experiment = 'experiment_scattering'\n",
        "!mkdir ./experiment_scattering\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 20\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 128\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 30\n",
        "\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "\n",
        "log_interval = 2\n",
        "\n",
        "input_size = 64\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lvVF3tuSG7d"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDmfilzqOfVK"
      },
      "source": [
        "# data transform\n",
        "\n",
        "data_transform = get_data_transform(input_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(data_dir + '/train_images',\n",
        "                         transform=data_transforms['train']),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(data_dir + '/val_images',\n",
        "                         transform=data_transforms['val']),\n",
        "    batch_size=batch_size, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3OSfZIm-0D_"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, scattering):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        sc =scattering(data)\n",
        "        output = model(sc)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader, scattering):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(scattering(data))\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQVjWbet_U9x"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "scattering = Scattering2D(J=2, shape=(input_size, input_size))\n",
        "K = 81*3\n",
        "if use_cuda:\n",
        "    scattering = scattering.cuda()\n",
        "\n",
        "model = CNNScattering(K).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7sjrtIYk_RK"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 81, 16, 16)) # the input is 81 channel because of the wavelet decomposition (and non-linearity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaje4rqiR-sS"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGgwBWmhk-dP"
      },
      "source": [
        "# Optimizer\n",
        "lr = 0.1\n",
        "for epoch in range(0, 90):\n",
        "    if epoch%20==0:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n",
        "                                    weight_decay=0.0005)\n",
        "        #lr*=0.2\n",
        "\n",
        "    train(model, device, train_loader, optimizer, epoch+1, scattering)\n",
        "    test(model, device, val_loader, scattering)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9rgirtAwQ-m"
      },
      "source": [
        "### kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf19KOy46sYb"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "from src.utils import pil_loader\n",
        "\n",
        "model.eval()\n",
        "test_dir = data_dir + '/test_images/mistery_category'\n",
        "\n",
        "output_file = open(f'{experiment}/result_scattering.csv', \"w\")\n",
        "output_file.write(\"Id,Category\\n\")\n",
        "for f in tqdm(os.listdir(test_dir)):\n",
        "    if 'jpg' in f:\n",
        "        data = data_transforms['val'](pil_loader(test_dir + '/' + f))\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "        sc = scattering(data)\n",
        "        output = model(sc)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
        "\n",
        "output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLWw5pNX7VU_"
      },
      "source": [
        "!git add experiment_scattering/result_scattering.csv\n",
        "!git commit -m 'add experiment with small cnn and scattering'\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}